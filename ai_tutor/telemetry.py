import time
import asyncio
from ai_tutor.dependencies import SUPABASE_CLIENT
from openai.types import CompletionUsage
from ai_tutor.context import TutorContext
from functools import wraps

# --- Background queue for Supabase writes ---
def enqueue(table: str, data: dict):
    if SUPABASE_CLIENT:
        async def write():
            SUPABASE_CLIENT.table(table).insert(data).execute()
        # Fire and forget
        asyncio.create_task(write())

def log_tool(fn):
    """
    Decorator that measures latency and writes an edge_logs row,
    *without* destroying the function_tool metadata.
    """
    @wraps(fn)
    async def wrapper(*args, **kwargs):
        start = time.perf_counter()
        res   = await fn(*args, **kwargs)
        ms    = int((time.perf_counter()-start)*1000)
        ctx   = args[0]               # every tool receives ctx first
        if SUPABASE_CLIENT:
            usage = getattr(res, "usage", None)
            enqueue("edge_logs", {
                "session_id": str(getattr(ctx, "session_id", None)),
                "user_id": str(getattr(ctx, "user_id", None)),
                "tool": fn.__name__,
                "latency_ms": ms,
                "prompt_tokens": getattr(usage, "prompt_tokens", None),
                "completion_tokens": getattr(usage, "completion_tokens", None),
                "trace_id": getattr(ctx, "trace_id", None),
            })
            # Persist context after every successful tool (still sync)
            if hasattr(ctx, "context") and isinstance(ctx.context, TutorContext):
                SUPABASE_CLIENT.table("sessions").update({
                    "context_json": ctx.context.model_dump()
                }).eq("id", str(ctx.context.session_id)).eq("user_id", str(ctx.context.user_id)).execute()
        return res
    # âœ¨ copy the JSON schema generated by @function_tool
    if hasattr(fn, "__ai_function_spec__"):
        wrapper.__ai_function_spec__ = fn.__ai_function_spec__
    return wrapper 